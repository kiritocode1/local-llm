{
  "name": "local-llm",
  "version": "0.1.0",
  "description": "Run LLMs directly in your browser with WebGPU acceleration",
  "module": "./src/index.ts",
  "main": "./dist/index.js",
  "types": "./dist/index.d.ts",
  "type": "module",
  "exports": {
    ".": {
      "import": "./dist/index.js",
      "types": "./dist/index.d.ts"
    },
    "./src": {
      "import": "./src/index.ts",
      "types": "./src/index.ts"
    }
  },
  "files": [
    "dist",
    "src"
  ],
  "scripts": {
    "dev": "bun --watch ./src/index.ts",
    "build": "bun build ./src/index.ts --outdir ./dist --format esm --sourcemap",
    "build:types": "tsc --declaration --emitDeclarationOnly --outDir dist",
    "demo": "bunx serve . -p 3000",
    "typecheck": "tsc --noEmit"
  },
  "keywords": [
    "llm",
    "ai",
    "browser",
    "webgpu",
    "webllm",
    "transformers",
    "local",
    "inference"
  ],
  "author": "",
  "license": "MIT",
  "devDependencies": {
    "@types/bun": "latest",
    "@webgpu/types": "^0.1.69"
  },
  "peerDependencies": {
    "typescript": "^5"
  },
  "dependencies": {
    "@huggingface/transformers": "^3.8.1",
    "@mlc-ai/web-llm": "^0.2.80"
  }
}
